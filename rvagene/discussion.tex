%what does it do?
%- visualization/classification (encoder) and prediction/generation (decoder)
%
%Plan
%- overview of main findings - ability to classify, predict, and generate 
%- current uses - tuning hyperparameters (DPGP) 
%- current uses - interpretability (cf. other VAEs)
%- future uses - interpretability - using LDVAE
%- future uses - (GRNs)
%- future work: time interval 
%- future work: discret time 
%- future work: different priors (cite SOUP)
%- conclusion


\section{Discussion}
{We have presented RVAgene, a recurrent variational autoencoder for generative modeling of gene expression time series data.
Through its encoder network, RVAgene provides means to visualize and classify gene expression dynamic profiles, which can lead to the discovery of biological processes.
Through its decoder network, RVAgene provides means to generate new gene expression dynamic profiles of either the full data or (in the case of single-cell studies) the pseudotime-smoothed data by sampling points from the latent space. In doing so, RVAgene can accurately reconstruct gene dynamics in complex biological data. As a by-product, on single-cell datasets the model directly produces smoothed outputs, useful for denoising gene expression time series data. RVAgene is efficient on temporally-rich whole genome datasets, in comparison to current existing methods. }
\par
RVAgene can be used to discover structure in the data, such as gene profile clusters. Popular
methods for clustering gene profiles such as Bayesian hierarchical clustering
\citep{cooke2011bayesian} or DPGP \citep{McDowell2018} detect the number of clusters in the data by
fitting a hyperparameter $\alpha$, the concentration parameter of the governing Dirichlet process
\citep{ferguson1973bayesian}. Although unsupervised, inevitably, the choice of $\alpha$ affects the
number of clusters output. Visualizing the data first with RVAgene can give an idea whether the data
favor clustering or a continuous representation. Thus analysis in RVAgene can guide the setting of
the hyperparameter $\alpha$ in DPGP and similar methods. In the case of ESC differentiation, DPGP
predicts 12 clusters (\hyperref[fig:figS3]{ Fig. S18}), yet most have very few members and many share
similar patterns. The RVAgene latent space for this dataset finds two major divisions in the data,
and orders the largest DPGP clusters along a spectrum (\hyperref[fig:fig3]{fig. 6.2D}), suggesting that DPGP might be overfitting the data. Indeed, the two methods can be used complementarily: RVAgene for high-level structure discovery and DPGP for clustering.
{In cases where learning a detailed noise model (at single time point resolution) is important to the user, DPGP or other Gaussian Process models are preferable over RVAgene.}
However, DPGP does not scale well with large datasets and thus cannot always be used
(\hyperref[fig:fig7]{fig. 6.6}).
\par 
The latent space of an RVAgene model encodes useful information about biological features, and in that sense provides biologically interpretable representations of the data. However, the representation is not interpretable in the sense that the components of the latent space do not have a physical meaning nor are they necessarily independent. Recent methods have tackled this issue of interpretability, by either modifying the loss function to make components independent \citep{higgins2016beta} or substituting linear functions in parts of the VAE \citep{svensson2020interpretable, ainsworth2018oi}. These methods have clear advantages regarding the analysis and interpretation of features in the latent space. In future work, decoding an RVAgene model with a linear function \citep{svensson2020interpretable} could facilitate additional discovery and improve our ability to gain insight into dynamic biological processes through the analysis of the latent space.
\par 
Dynamic changes in gene expression underlie essential cell processes. As such, modeling gene expression changes can also facilitate downstream analysis tasks, including gene regulatory network (GRN) inference. Inferring gene regulatory networks from single-cell data is challenging \citep{chen18_evaluating}, particularly due to cell-cell heterogeneity and high levels of noise. Several recent approaches to GRN inference make use of temporal profiles \citep{deshpande19_network, kim20_tenet}  
or differential equations \citep{ma20_inference, aubin-frankowski20_gene, matsumoto17_scode}. RVAgene could supplement such methods either by providing denoised input data, or by completely replacing the temporal ordering/differential equation-based components of these methods (which can be notoriously difficult to parameterize) with data produced from a RVAgene generative model of the gene expression dynamics.
\par 
RVAgene is currently agnostic of irregular time intervals between consecutive points in a time series,  i.e. it standardizes the time interval. This is not usually a concern for single-cell data, since with pseudotime information we can choose appropriate time intervals. However, in other cases, such as in response to kidney injury \citep{liu2017molecular}, standardizing time intervals distorts the dynamic profiles. Since RVAgene seeks to describe broad temporal patterns, we do not see this as a critical issue, though it would be desirable to generalize the model. A simple way to model irregularly spaced time points would be to augment the data through interpolation, though this is difficult without making strong assumptions about the (generally unknown) noise model. Gaussian process models \citep{McDowell2018, hensman2013hierarchical} can take irregular data as input, although (as noted above) are not efficient enough to run on large datasets. An alternative approach would be to modify the recurrent network architecture to take time points explicitly as input values, this would enable modeling of irregular or asynchronous data \citep{wu2018modeling}.
%\tcr{ https://link.springer.com/article/10.1186/1471-2105-14-252}
\par
RVAgene models in discrete time steps. There is no simple modification to the recurrent network structure that allows for prediction on continuously valued time. However, a recent development: neural ordinary differential equations (ODEs) \citep{chen2018neural}, enables modeling of time series data with continuous timepoints. \citet{chen2018neural} describe a generative latent ODE architecture similar to that of RVAgene, except that in their case the recurrent decoder network is replaced by a neural ODE decoder network. \citet{chen2018neural} demonstrate accurate results using synthetic data, however when we applied the method to the ESC single-cell differentiation dataset \citep{Klein2015}, the neural ODE network was found to converge very slowly and was overall underfit (\hyperref[fig:figS9]{Fig. S24}). The latent ODE method used by \citet{chen2018neural} does not address the challenge of modeling asynchronous/irregularly spaced data, but this has been more recently addressed \citep{rubanova2019latent}. These new models may well lead to future improvements in network architectures, although it seems that computational progress is needed before they can be successfully applied to complex biological systems.
%We will keenly follow developments in the field. 
\par 
In the current work, the prior on latent space used throughout was a unit spherical Normal, appropriate for exploratory data analysis where we have no further knowledge about structure in the latent space.
% ($\cN(\mathbf{0},\mathbf{I})$). 
However, given more information, e.g. that the data contains $k$ clusters, a different prior on the latent space might be more appropriate. A multi-modal prior -- such as a Gaussian Mixture Model (GMM) prior -- would permit structured (multi-modal) representations. However, the KL-divergence for an arbitrary GMM is not tractable; approximation  \citep{hershey2007approximating} or numerical computation would be necessary. Moreover, there is a greater problem: mixture models contain discrete parameters and VAE models are ill-suited for the optimization of discrete parameters \citep{dilokthanakul2016deep}, thus directly replacing the Normal prior of a VAE with a GMM is not feasible. A workaround to this problem is presented in \citep{dilokthanakul2016deep}, however implementing this for a recurrent model architecture remains an open problem. 
%A  k-modal prior would increase the probability of a latent space representation containing $\geq k$ clusters, i.e. increasing the structure of the space. 
%cite SOUP
\par 
The points raised above offer much scope for future work. These include the design of new latent space models with informative priors, modeling irregular time series data, and modeling in continuous time. Developments in some of these areas \citep{chen2018neural}, while promising, tend to rely on training data with relatively low levels of noise: far from the reality of most biological data. Thus it seems highly likely to be beneficial for both machine learning and biology to develop new neural network architectures in light of biological data. 








