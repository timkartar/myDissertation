\section{Proposed Method} 
In chapter 1, we learned a generative model of gene expression time
series data using a VAE framework, where we learned a regularized latent space representation of latent
variables $z$ by reconstructing the data $x$ through an encoder-decoder architecture. We can start
thinking about the binding element design problem in the same manner.

Let's define $x_p$ as input binding site information. We want to predict a binding element $x_e$
given $x_p$. To achieve this task, we again formulate a generative story: $x_e$ is generated from
latent variables $z$, which can be modeled by a decoder/generator network which models the
distribution $P(x_e|z)$. And we can employ an encoder netowk modeling $P(z|x_p)$ to achieve the
latent information $z$ from the input $x_e$.

Now, to formally achieve the prediction task, we start with writing out the conditional distribution,
$P(x_e, z|x_p)$ which we would like to maximize and predicts the argument $x_e$ and $z$ that maximizes it.

\begin{align*}
P(x_e,z|x_p) = P(x_e|z,x_p)P(z|x_p) \numberthis
\end{align*}
Here, we assume a hierarchical bayesian setting where given the latent variable $z$, $x_e$ is
generated independent of $x_p$ i.e. $P(x_e|z,x_p) = P(x_e|z)$. Now we can write the following,
\begin{align*}
P(x_e,z|x_p) &= P(x_e|z)P(z|x_p) \numberthis \\
\implies \text{prediction } \hat{x}_e, \hat{z} &= argmax_{x_e,z} P(x_e,z|x_p) \numberthis \\
&= argmax_{x_e,z} P(x_e|z)P(z|x_p)  \\
\implies  \hat{z} = argmax_{z} P(z|x_p); & \;\;\;\;\hat{x}_e = argmax_{x_e} P(x_e | \hat{z}) \numberthis \\
\implies \hat{z} = \cE(x_p); & \;\;\;\; \hat{x}_e = \cG(\hat{z}) \numberthis
\label{final_prediction_architecture}
\end{align*}
The functions $\cE(x_p)$ and $\cG(\hat{z})$ in \ref{final_prediction_architecture} represents the
encoder and generator functions respectively both of which are parametrized by neural networks. 

Now, the encoder-generator framework described above is suitable for prediction, however it's
difficult to train it in a straight forward manner. For RVAgene \red{(cite)} we were able to use
reconstruction based method to train becaue the input of encoder and output of decoder was the same.
Here, they are different. This constitutes a proper scenario to employ an adversarial training
scheme as employed by GANs \citep{goodfellow2014generative}.

Given a training dataset $X$ consisting of $n$ datapoints where $X_i = (x_p^i, x_e^i) \;\; i \in
\{0,..., n-1\}$, we now describe
how to train the generative model described in \ref{final_prediction_architecture}. We assign a
label variable $y = 1$ for all datatpoints in the training data $X$. Now, assume we have some
datapairs $F = (x_p^i, \hat{x}_e^i) \;\; i \in \{0,..., m-1\}$ generated by the encoder-generator
system given $x_p^i$s as input. We assign these datapoints a label $y = 0$. Now, we can train a
neural network $\cD$ discriminating between the datasets X and F i.e. the discriminator $\cD$ is
predicting $P(y = 1 | x_p, x_e)$. It isclear that the better the enocder-generator system is in
predicting binding element given a protein surface the harder the job of the discriminator becomes.
We take advantage of this fact and teach the encoder-generator network to try to best the
discriminato and vice versa. The two networks play the following minimax game:
\begin{align*}
min_{\cG,\cE}max_{\cD} V(D, E, G) = \bE_{x_p, x_e \sim P_X}\bigg{[}log D(x_p, x_e)\bigg{]} + \bE_{x_p \sim
P_{x_p}}\bigg{[}log(1 - D(x_p,G(E(x_p))))\bigg{]} \numberthis \label{gan_game}
\end{align*}
In eq. \ref{gan_game} above $P_X$ represents the full training data distribution and $P_{x_p}$
represents the marginal distribution of binding sites in the trainning data. 

If we call the generated distribution of $x_p, \hat{x}_e$ as $p_g$, as described and proven in
\citet{goodfellow2014generative} this adversarial game 
eventually results into the generated data distribution becoming same as the training data
distribution $P_x$, which
happens at the point when both the $(\cE, \cG)$ and $\cD$ network cannot improve anymore.
